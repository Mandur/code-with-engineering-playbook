## Observability in Machine Learning

Systems with machine learning model component 

### Model experimentation and tuning : logging and dashboard
<here is about metrics logging, comparison variuos models, dashboards>
When developing and tuning machine learning models data scientist experiments with various parameters
and model performance metrics. 
Tools that facilitate team of data scientists to observe and share modelling results are.

[MLFlow for Databricks](https://docs.microsoft.com/en-us/azure/databricks/applications/mlflow/)
<here is best practice to observe model behaiviour using MLFlow>

[Azure Machine learning Service](https://ml.azure.com/)
<here is best practice to observe model behaiviour using AML>

### Model experimentation and tuning: reproducibility  

< here is about reproducible experimenation>  

### Model performance over time: data drift
In re-tranining scenario model behaviour may degrade due to changes in data.
<here is how to observe and report model performance in production>  

    Model deployment:
    Deployed as dependency (pickle file)  
    Deployed as service


### Data versioning









